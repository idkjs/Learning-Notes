# AWS S3

* [s4cmd](https://github.com/bloomreach/s4cmd)
* [Alestic - A personal AWS blog](https://alestic.com/)
* [jq play](https://jqplay.org/)
* [10 things you might not knowing using S3](https://www.sumologic.com/aws/s3/10-things-might-not-know-using-s3/)
* [Sumo Logic - Log Analytics](https://www.sumologic.com/application/s3/)
* [ZBackup - A versatile deduplicating backup tool](https://github.com/zbackup/zbackup)
* [restic - Fast, secure, efficient backup program](https://github.com/restic/restic)

What is Object Storage?

* No hierarchy. All objects exist at the same level.
* Globally unique ID. No need to know the physical location of the data.
* Not a filesystem, rather a key-value store
* Scale to very high request rates. If the request rate grows steadily, S3 automatically partitions the buckets as needed to support higher request rates.
* 5GB upload limit and 5TB storage limit

If you want to cut cost, you can use reduced redundancy with just 4 nines (99.99%) instead of 11 nines.

**3 storage classes:**

1. S3 Standard - Milliseconds
2. S3 Infrequent Access - Milliseconds, min 128KB size. Cheaper to store, but expensive to access.
3. Glacier - 4 hours. Much more cheaper to store but even more expensive to access.

## aws-cli

```
▶ brew install awscli
▶ aws configure

▶ aws s3 ls s3://jobline-assets/avatars/ | wc -l
▶ aws s3 ls --recursive s3://jobline-assets/claims/ | wc -l
▶ aws s3 ls --recursive s3://jobline-assets/timesheets/ | wc -l

// Last check count Feb 15, 2016
39411
10923
5335

▶ aws iam list-users
▶ aws sts get-caller-identity
▶ aws ec2 describe-instances | grep Dns

// Parsing using the excellent jq
▶ aws route53 list-hosted-zones | jq '.HostedZones[].Name'

▶ aws s3 cp <local_folder> s3://<bucket><remote_folder> --recursive --exclude ".DS_Store"

▶ aws s3 cp /path/to/folder s3://bucket/folder --recursive

▶ aws s3api get-object --bucket my-bucket --key inv/test.pdf local-name.pdf

// Use Content-MD5 Header for S3 putObject
▶ aws s3api put-object --bucket my-bucket --key inv/test.pdf --body localfile.pdf --metadata md5chksum=XXX --content-md5 XXX
```

```bash
#!/bin/sh

CURDATE=`date +%d%m%y%H%M`

# create backup file
tar czf /var/www/backups/ ...

# remove backups older than 7 days
find /var/www/backups/portalbackup* -mtime +7 -exec rm {} \;

# sync to S3
/usr/local/bin/aws s3 sync /var/www/backups s3://my-bucket --delete
```

## Lifecycle Policies

Exist to help you manage storage costs.

* Automatically transition between storage classes
* Automatically delete objects (good for deleting old database backups) to save costs
* Log file usually decrease in usefulness as it get older. You may want to access them when they are fresh, but become increasingly useful as time goes on, and eventually need to be deleted after serving its compliance years.
* Store original avatars at Reduced Redundancy and thumbnail at Standard CDN
* Make use of **Object Tagging** to filter objects for transition

## Key Naming Scheme (Partitions)

> GUIDs "fail" for S3 hashing. First 4 characters are usually the same.

Also apply to bucket name??

The object names you choose actually dictate how S3 manage the keymap.

Keys in S3 are **partitioned by prefix**. S3 has automation that continually looks for areas of the keyspace that need splitting. Partitions are split either due to sustained high request rates, or because they contain a large number of keys (which would slow down lookups within the partition).

Using a sequential prefix, such as timestamp or an alphabetical sequence, increases the likelihood that S3 will target a specific partition for a large number of keys, overwhelming the I/O capacity of the partition.

Smart naming of keys can have performance implications. But only if you have more than **100 TPS (100/8 = 12.5 events/sec)**.

You can introduce random hash or reverse object ID to evenly distribute your keys so it won't overwhelm any single partition:

* Introduce 4-character random hexadecimal hash
* Reverse any ID (i.e. from CL8375 to 5738LC)

```
// Bad
bucketname/2013-26-05-15-00-00/cust1234234/photo1.jpg
bucketname/2013-26-05-15-00-00/cust3857422/photo2.jpg
bucketname/2013-26-05-15-00-00/cust1248473/photo2.jpg

// Good
bucketname/232a-2013-26-05-15-00-00/cust1234234/photo1.jpg
bucketname/7b54-2013-26-05-15-00-00/cust3857422/photo2.jpg
bucketname/921c-2013-26-05-15-00-00/cust1248473/photo2.jpg

// S3 can use the first character of the key name for partitioning
bucketname/2
bucketname/7
bucketname/9

// but for very large workloads, S3 can use more characters for the partitioning scheme
bucketname/22
bucketname/7b
bucketname/92

// Store objects as a hash of their name
// Add the original name as metadata
deadmau5_mix.mp3 -> 0aa316fb000eae52921aabb1b2697522344ad99

// Prepend key name with short hash
0aa3-deadmau5_mix.mp3

// Epoch time (reverse)
5321354831-deadmau5_mix.mp3
```

> Amazon S3 can use the **first character of the key name** for partitioning, but for very large workloads (more than 2000 requests per seconds or for bucket that contain billions of objects), Amazon S3 can use **more characters** for the partitioning scheme. Amazon S3 can automatically split these partitions further as the key count and request rate increase over time.

For **non-CDN objects**, you want to introduce randomness in order not to overwhelm the **single index partition**.

**Note:** For Jobline avatar or any assets to be downloaded by client, using Amazon CloudFront CDN is a much better option. It will reduce cost also since it send fewer requests directly to S3.

It is interesting to know that the hash does not need to be at the beginning of the key, we can have many similar prefix before that and have the hash in the middle:

```
bucketname/images/2321-2134555/photo1.jpg
bucketname/images/7b54-2134858/photo2.jpg
```

**Randomness in key names can be anti-pattern**

* Lifecycle policies difficult to do
* LISTs with prefix filters
* Maintaining thumbnails of images
* When you need to recover a file with its original name

To solve the anti-pattern, we can add additional prefixes to help sorting.

## Sorting and Listing

Key names are stored lexicographically in S3 indexes, making it hard to sort and manipulate the contents of `LIST`.

We can build and maintain a secondary index outside of S3 (e.g. DynamoDB or Elasticsearch) to store, index and query objects metadata rather than performing operations directly on S3.

## Security

Does your application need to write to an S3 bucket? Use a **IAM Role** instead of IAM User. IAM Roles reduce the risk of someone stealing a set of credentials by avoiding having to create those credentials in the first place.

* [How to Restrict Amazon S3 Bucket Access to a Specific IAM Role](https://aws.amazon.com/blogs/security/how-to-restrict-amazon-s3-bucket-access-to-a-specific-iam-role/)
* [From dev to production: Security best practices on managing AWS environments](https://www.safaribooksonline.com/library/view/oreilly-security-conference/9781491976128/video290067.html)
* [Murder in the Amazon cloud](http://www.infoworld.com/article/2608076/data-center/murder-in-the-amazon-cloud.html)
* CIS AWS Foundations Benchmark
* Security Monkey
* Do account separation (i.e. dev, pro, finance, etc.)

S3 provides very durable data storage. But all that redundancy won't help if you accidentally delete data or if someone else maliciously deletes data.

Use **Versioning** and **MFA Delete** to protect your data.

* Versioning to protect from unintended overwrites. A simple `DELETE` operation cannot actually delete an object from your bucket. All it does is create a special delete marker that shows the object was removed; you can still retrieve it by using its name and version ID. You need to perform a "versioned delete" that specifies both the object name and version ID. This makes accidental delete much less likely.
* Enable MFA on bucket to prevent object deletion
* Set up policy to deny `s3:Delete*`
* Enable CloudTrail to log events

```
▶ aws cloudtrail delete-trail --name [trail-name]
▶ aws cloudtrail stop-logging --name [trail-name]

// List all events for the last 7 days
▶ aws cloudtrail lookup-events --output json

// List all events where user name is root
▶ aws cloudtrail lookup-events --lookup-attributes \
AttributeKey=Username, AttributeValue=root --output=json
```

## Versioning

* Protect from accidental deletion 

## Metadata

If you want to see if an object exists or not, instead of downloading the whole object, you can query just for its metadata.

## Object Tags

* Classify your data
* Write policies
* 10 tags per object
* Mutable
* $0.01 per 10,000 tags per month

## CloudWatch Metric and CloudTrail

* To see your S3 storage performance
* See who modify data

## Policy

* [IAM Policy Simulator](https://policysim.aws.amazon.com/home/index.jsp?#)
* [Writing IAM Policies: How to Grant Access to an Amazon S3 Bucket](https://aws.amazon.com/blogs/security/writing-iam-policies-how-to-grant-access-to-an-amazon-s3-bucket/)
* [SO: Is there an S3 policy for limiting access to only see/access one bucket?](https://stackoverflow.com/questions/6615168/is-there-an-s3-policy-for-limiting-access-to-only-see-access-one-bucket)
* [All S3 Actions](https://docs.aws.amazon.com/AmazonS3/latest/dev/using-with-s3-actions.html)

```json
[
  "s3:GetObject",
  "s3:GetBucketLocation",
  "s3:ListBucket",
  "s3:ListAllMyBuckets",
]
```

**AmazonS3ReadOnlyAccess**

```json
{
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:Get*",
        "s3:List*"
      ],
      "Resource": "*"
    }
  ]
}
```

**No-Delete Policy:**

```json
{
  "Statement": [
  
    {
      "Effect": "Deny",
      "Action": [
        "s3:DeleteBucket",
        "s3:DeleteBucketPolicy",
        "s3:DeleteBucketWebsite",
        "s3:DeleteObject",
        "s3:DeleteObjectVersion",
        "s3:DeleteObjectTagging",
        "s3:DeleteObjectVersionTagging",
      ],
      "Resource": [
        "arn:aws:s3:::*"
      ]
    },
    
    {
      "Effect": "Allow",
      "Action": "s3:ListAllMyBuckets",
      "Resource": "*"
    }
    
  ]
}

```

**Restricting access to specific IP addresses:**

```json
{
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:*",
      "Principal": "*",
      "Resource": "arn:aws:s3:::examplebucket/*",
      "Condition": {
        "IpAddress": {"aws:SourceIp": "54.240.143.0/24"},
        "NotIpAddress": {"aws:SourceIp": "54.240.143.188/32"}
      }
    }
  ]
}
```

**User could not perform any actions with no HTTPS and correct IP**

```json
{
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:*",
      "Resource": "arn:aws:s3:::db-backups*",
      "Condition": {
        "Bool": {
          "aws:SecureTransport": "true"
        },
        "IpAddress": {
          "aws:SourceIp": "192.168.10.10"
        }
      }
    }
  ]
}
```

## Bucket Policy

In additional to IAM policy, your bucket can also have its own policy to have the final say on who can access it.

**Allow public access**

```json
{
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": "*",
      "Action": [
        "s3:GetObject"
      ],
      "Resource": "arn:aws:s3:::examplebucket/*"
    }
  ]
}
```

**Enforce Server-Side Encryption**

```json
{
  "Statement": [
    {
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:PutObject",
      "Resource": "arn:aws:s3:::db-backups/*",
      "Condition": {
        "StringNotEquals": {
          "s3:x-amz-server-side-encryption": "AES256"
        }
      }
    },
    {
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:PutObject",
      "Resource": "arn:aws:s3:::db-backups/*",
      "Condition": {
        "Null": {
          "s3:x-amz-server-side-encryption": true
        }
      }
    }
  ]
}
```

## Multipart Upload

Parallelizing PUTs with multipart uploads. Increase aggregate throughput by parallelizing PUTs on high-bandwidth networks.

## LISTs

Using secondary indexes with external database to replace LIST requests.

* Simplify your code
* Enable distributed processing
* Segment and filter the objects you need to GET

> PUTs and LISTs are an easy-to-overlook expense!

## CloudFront

S3 is not intended to solve latency!

## Videos

* [AWS re:Invent 2014 - Maximizing Amazon S3 Performance](https://www.youtube.com/watch?v=_FHRzq7eHQc)