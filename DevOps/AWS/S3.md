# AWS S3

What is Object Storage?

* No hierarchy. All objects exist at the same level.
* Globally unique ID. No need to know the physical location of the data.
* Not a filesystem, rather a key-value store
* Scale to very high request rates. If the request rate grows steadily, S3 automatically partitions the buckets as needed to support higher request rates.

If you want to cut cost, you can use reduced redundancy with just 4 nines (99.99%) instead of 11 nines.

**3 storage classes:**

1. S3 Standard - Milliseconds
2. S3 Infrequent Access - Milliseconds, min 128KB size. Cheaper to store, but expensive to retrieve.
3. Glacier - 4 hours

## aws-cli

```
▶ brew install awscli
▶ aws configure

▶ aws s3 ls s3://jobline-assets/avatars/ | wc -l
▶ aws s3 ls --recursive s3://jobline-assets/claims/ | wc -l
▶ aws s3 ls --recursive s3://jobline-assets/timesheets/ | wc -l

// Last check count Feb 15, 2016
39411
10923
5335
```

## Lifecycle Policies

Exist to help you manage storage costs.

* Automatically transition between storage classes
* Automatically delete objects (good for deleting old database backups) to save costs
* Log file usually decrease in usefulness as it get older. You may want to access them when they are fresh, but become increasingly useful as time goes on, and eventually need to be deleted after serving its compliance years.
* Store original avatars at Reduced Redundancy and thumbnail at Standard CDN

## Key Partitions

Also apply to bucket name??

The object names you choose actually dictate how S3 manage the keymap.

Keys in S3 are **partitioned by prefix**. S3 has automation that continually looks for areas of the keyspace that need splitting. Partitions are split either due to sustained high request rates, or because they contain a large number of keys (which would slow down lookups within the partition).

Using a sequential prefix, such as timestamp or an alphabetical sequence, increases the likelihood that S3 will target a specific partition for a large number of keys, overwhelming the I/O capacity of the partition.

Smart naming of keys can have performance implications. But only if you have more than **100 request/second**.

You can introduce random hash or reverse object ID to evenly distribute your keys so it won't overwhelm any single partition:

* Introduce 4-character random hexadecimal hash
* Reverse any ID (i.e. from CL8375 to 5738LC)

```
// Bad
bucketname/2013-26-05-15-00-00/cust1234234/photo1.jpg
bucketname/2013-26-05-15-00-00/cust3857422/photo2.jpg
bucketname/2013-26-05-15-00-00/cust1248473/photo2.jpg

// Good
bucketname/232a-2013-26-05-15-00-00/cust1234234/photo1.jpg
bucketname/7b54-2013-26-05-15-00-00/cust3857422/photo2.jpg
bucketname/921c-2013-26-05-15-00-00/cust1248473/photo2.jpg

// S3 can use the first character of the key name for partitioning
bucketname/2
bucketname/7
bucketname/9

// but for very large workloads, S3 can use more characters for the partitioning scheme
bucketname/22
bucketname/7b
bucketname/92
```

> Amazon S3 can use the **first character of the key name** for partitioning, but for very large workloads (more than 2000 requests per seconds or for bucket that contain billions of objects), Amazon S3 can use **more characters** for the partitioning scheme. Amazon S3 can automatically split these partitions further as the key count and request rate increase over time.

For **non-CDN objects**, you want to introduce randomness in order not to overwhelm the **single index partition**.

**Note:** For Jobline avatar or any assets to be downloaded by client, using Amazon CloudFront CDN is a much better option. It will reduce cost also since it send fewer requests directly to S3.

It is interesting to know that the hash does not need to be at the beginning of the key, we can have many similar prefix before that and have the hash in the middle:

```
bucketname/images/2321-2134555/photo1.jpg
bucketname/images/7b54-2134858/photo2.jpg
```

## Sorting and Listing

Key names are stored lexicographically in S3 indexes, making it hard to sort and manipulate the contents of `LIST`.

We can build and maintain a secondary index outside of S3 (e.g. DynamoDB or Elastisearch) to store, index and query objects metadata rather than performing operations directly on S3.

## Security

* Versioning to protect from unintended overwrites and deletions
* Enable MFA on bucket to prevent deletion

## Metadata

If you want to see if an object exists or not, instead of downloading the whole object, you can query just for its metadata.

## Object Tags

* Classify your data
* Write policies
* 10 tags per object
* Mutable
* $0.01 per 10,000 tags per month

## CloudWatch Metric and CloudTrail

* To see your S3 storage performance
* See who modify data

## Policy

Restricting access to specific IP addresses:

```json
"Statement": [
  {
    "Effect": "Allow",
    "Principal": "*",
    "Action": "s3:*",
    "Resource": "arn:aws:s3:::examplebucket/*",
    "Condition": {
      "IpAddress": {"aws:SourceIp": "54.240.143.0/24"},
      "NotIpAddress": {"aws:SourceIp": "54.240.143.188/32"}
    }
  }
]
```